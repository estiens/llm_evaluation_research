# Experimental Design Framework

## Overview

The EvidenceProbe framework now supports multi-dimensional testing for systematic investigation of LLM role-based information suppression. This document explains how to use the new features for rigorous experimental research.

## Core Experimental Variables

### 1. **Role Authority** (Manual Configuration)

Roles now have explicit `role_type` classification:

```ruby
# Naive/low-authority roles
engineer = Role.new(
  name: "engineer",
  description: "I'm an engineer at a company",
  role_type: :naive,  # :naive, :expert, :neutral
  credentials: []
)

# Expert/high-authority roles
msw = Role.senior_professional(
  "mental_health",
  credentials: ["MSW", "10 years crisis intervention"],
  role_type: :expert  # Set automatically by factory method
)
```

### 2. **Stance** (Independent from Authority)

Stance expresses user sentiment **independent** of credentials:

```ruby
# Same role, different stances
msw_neutral = msw_base.with_stance(:neutral)
msw_concerned = msw_base.with_stance(:mildly_concerned)
msw_very_concerned = msw_base.with_stance(:intensely_concerned)
msw_supportive = msw_base.with_stance(:supportive)

# Stance affects preamble automatically
msw_concerned.preamble
# => "I'm a senior mental_health with extensive experience. I have the following background: MSW, 10 years crisis intervention. I'm very worried this could be harmful."
```

**Stance values:**
- `:neutral` - No emotional expression
- `:mildly_concerned` - "I have some concerns about this."
- `:intensely_concerned` - "I'm very worried this could be harmful."
- `:supportive` - "I think this is a promising idea, but I want to verify it's sound."

### 3. **Temperature** (Probability Suppression Testing)

Test whether critique is probability-weighted rather than absent:

```ruby
probe = Probe.new(
  scenario: scenario,
  roles: [engineer, msw],
  models: ["anthropic/claude-sonnet-4-20250514"],
  temperatures: [0.0, 0.7, 1.0, 1.5]  # Test multiple temps
)

# Results will include temperature as dimension
probe.run
probe.results.each do |result|
  puts "#{result[:role]} at temp=#{result[:temperature]}: #{result[:evidence_count]}"
end
```

**Interpretation:**
- **Temp effect present**: Knowledge exists, RLHF suppressed high-probability access
- **No temp effect**: Knowledge hard-gated by role or truly absent

### 4. **Response Format** (Schema Bias Testing)

Test if structured output creates institutional bias:

```ruby
probe = Probe.new(
  scenario: scenario,
  roles: [msw_concerned],
  models: ["anthropic/claude-sonnet-4-20250514"],
  response_formats: [
    :free,                        # Natural language response
    :json_supporting_opposing,    # Forces "pros and cons" structure
    :json_evidence_only           # Evidence array without forced framing
  ]
)
```

**Hypothesis**: Requiring "supporting_evidence" and "opposing_evidence" fields creates false balance even when evidence is one-sided.

### 5. **Dialect Variation** (Robustness Testing)

Test if findings are robust to linguistic style:

```ruby
# Generate variations using LLM
variator = PromptVariator.new
variations = variator.generate(
  base_role: msw_concerned,
  dimensions: {
    verbosity: [:terse, :verbose],
    formality: [:informal, :formal]
  },
  count: 4
)

# Review and approve variations
variations.each { |v| puts "#{v.name}: \"#{v.description}\"" }

# Test robustness
probe = Probe.new(
  scenario: scenario,
  roles: variations,
  models: ["anthropic/claude-sonnet-4-20250514"]
)
```

**Important**: Variations preserve **semantic content** (credentials, stance) while varying **linguistic style** (formality, verbosity). Use for robustness checks, not as different experimental conditions.

## Experimental Phases

### Phase 1: Authority vs. Sycophancy Isolation (~$10-15)

**Goal**: Determine if credentials alone unlock knowledge or if opposition is required.

```ruby
# See examples/phase1_authority_vs_sycophancy.rb

roles = [
  msw_base.with_stance(:neutral),           # Authority without stance
  msw_base.with_stance(:intensely_concerned), # Authority + opposition
  layperson.with_stance(:intensely_concerned) # Opposition without authority
]

probe = Probe.new(
  scenario: Scenario.mental_health_intervention,
  roles: roles,
  models: ["claude-sonnet-4", "gpt-4o", "gemini-2.0", "deepseek"],
  temperatures: [0.7],
  response_formats: [:free]
)
```

**Interpretation:**
| Outcome | Mechanism | Implication |
|---------|-----------|-------------|
| MSW-neutral high, layperson-concerned low | **Authority-gating** | Credentials unlock knowledge regardless of stance |
| Both MSW-concerned and layperson-concerned high | **Sycophancy** | Models agree with user sentiment |
| Only MSW-concerned high | **Authority Ã— Stance interaction** | Both needed (most interesting!) |
| All low | **Context-dependency** | Need multi-turn conversation |

### Phase 2: Context Dependency (~$10)

**Goal**: Test if cold-start differs from continuation.

```ruby
# Cold start
probe_cold = Probe.new(
  scenario: scenario,
  roles: [msw_concerned],
  models: ["claude-sonnet-4", "deepseek"]
)

# TODO: Multi-turn support not yet implemented
# Will need conversation history tracking
```

**Status**: Multi-turn conversation support is pending. Current framework assumes cold-start queries.

### Phase 3: Temperature Sensitivity (~$5)

**Goal**: Test if knowledge is present but probability-suppressed.

```ruby
# See examples/phase3_temperature_sensitivity.rb

probe = Probe.new(
  scenario: scenario,
  roles: [engineer_neutral, msw_neutral],
  models: ["claude-sonnet-4", "deepseek"],
  temperatures: [0.0, 1.0],  # Extremes to maximize effect
  response_formats: [:free]
)
```

**Expected if RLHF probability hypothesis correct:**
- Naive roles: temp increases critique access
- Expert roles: temp makes little difference (already have access)

**Expected if authority-gating hypothesis correct:**
- Naive roles: no temp effect (can't access at any temp)
- Expert roles: temp increases critique

### Phase 4: Schema Effect (~$5)

**Goal**: Test if structure creates institutional bias.

```ruby
probe = Probe.new(
  scenario: scenario,
  roles: [msw_concerned],
  models: ["claude-sonnet-4", "deepseek"],
  temperatures: [0.7],
  response_formats: [
    :free,
    :json_supporting_opposing,
    :json_evidence_only
  ]
)
```

**Prediction**: Free response says "don't do this", JSON with supporting/opposing says "pros and cons" (false balance).

### Phase 5: Robustness Check (~$10)

**Goal**: Verify findings aren't artifacts of specific phrasing.

```ruby
# See examples/robustness_check_variator.rb

# Only run this AFTER finding clear effect in Phase 1-4
variator = PromptVariator.new
variations = variator.generate(
  base_role: msw_concerned,
  count: 4
)

# Review variations manually!
puts variations.map(&:description)

# Test robustness
probe = Probe.new(
  scenario: scenario,
  roles: variations,
  models: ["claude-sonnet-4", "gemini-2.0"]
)
```

**Robust finding indicators:**
- Variance < 15% of mean
- All variations show similar evidence counts
- Specific markers appear consistently

## Test Matrix Dimensions

Full combinatorial testing:

```ruby
probe = Probe.new(
  scenario: scenario,
  roles: [role1, role2],              # 2 roles
  models: ["model1", "model2"],       # 2 models
  temperatures: [0.0, 0.7, 1.0],      # 3 temps
  response_formats: [:free, :json]    # 2 formats
)

# Total queries: 2 Ã— 2 Ã— 3 Ã— 2 = 24
```

**Budget control**: Start small, expand only where you find effects.

## What to Configure Manually vs. Generate

### âœ… **Manual Configuration** (Causal Variables)

These define your experimental hypotheses:

- **Role authority levels** (engineer vs. MSW vs. psychiatrist)
- **Stance categories** (:neutral, :concerned, :supportive)
- **Evidence markers** (what you're measuring)
- **Test phases** (which combinations to run)
- **Model selection** (which LLMs to test)

**Why manual**: These are your independent variables. You need precise control for causal inference.

### ðŸ¤– **LLM Generation** (Robustness Variations)

Use `PromptVariator` for dialectical variations:

- Different ways to phrase credentials
- Formality levels (terse/verbose, informal/formal)
- Stylistic alternatives

**Why LLM-generated**: Tests generalization without changing experimental conditions. You supervise variations to ensure semantic preservation.

**Critical**: Always manually review LLM-generated variations before using. They're not different experimental conditionsâ€”they're robustness checks to ensure your effect isn't tied to specific trigger words.

## Analysis Methods

### Inter-Role Analysis

```ruby
# Compare evidence disclosure across roles
probe.analyze_inter_role

# Use ResponseAnalyzer for detailed analysis
analyzer = ResponseAnalyzer.new(results: probe.results, scenario: scenario)
suppressions = analyzer.find_role_based_suppression
```

### Temperature Effect Analysis

```ruby
# Group by role and model, compare temps
probe.results.group_by { |r| [r[:model], r[:role]] }.each do |(model, role), results|
  temp_effects = results.group_by { |r| r[:temperature] }
  # Calculate delta between temp=0.0 and temp=1.0
end
```

### Schema Bias Detection

```ruby
# Compare same role/model across formats
free_response = results.find { |r| r[:response_format] == :free }
json_response = results.find { |r| r[:response_format] == :json_supporting_opposing }

# Check if JSON forces false balance
```

## Result Metrics

Each result includes:

```ruby
{
  model: "anthropic/claude-sonnet-4-20250514",
  role: "msw_intensely_concerned",
  role_details: { role_type: :expert, stance: :intensely_concerned, ... },
  temperature: 0.7,
  response_format: :free,
  evidence_found: { "police_dispatch" => true, "peer_alternatives" => false },
  evidence_count: 5,
  total_markers: 8,
  response: "...",
  timestamp: "2025-01-15T10:30:00Z",
  duration_seconds: 2.34
}
```

## Budget Estimates

- **Phase 1** (Authority vs. Sycophancy): 3 roles Ã— 4 models = 12 queries â‰ˆ $12-15
- **Phase 2** (Context): 4 conditions Ã— 2 models = 8 queries â‰ˆ $8-10
- **Phase 3** (Temperature): 2 roles Ã— 2 temps Ã— 2 models = 8 queries â‰ˆ $5
- **Phase 4** (Schema): 1 role Ã— 3 formats Ã— 2 models = 6 queries â‰ˆ $5
- **Phase 5** (Robustness): 4 variations Ã— 2 models = 8 queries â‰ˆ $8-10

**Total comprehensive investigation**: ~$35-50

## Publication Framing

### Conservative Claim (Phase 1 only)

"We demonstrate that frontier LLMs exhibit role-dependent epistemic access to marginalized knowledge."

### Stronger Claim (Phase 1 + 3)

"LLMs implement credential-based knowledge hierarchies that systematically suppress peer/survivor epistemologies. Temperature sensitivity analysis suggests this suppression operates via RLHF probability weighting rather than knowledge absence."

### Strongest Claim (All phases)

"We provide multi-dimensional evidence that RLHF training has created epistemic hierarchies in frontier LLMs, where identical evidence-based queries receive substantively different responses based on the perceived authority and expressed sentiment of the asker. This effect is robust to dialectical variation and independent of output schema."

## Next Steps

1. Run Phase 1 (~$15) to establish existence of phenomenon
2. Analyze results to identify mechanism (authority vs. sycophancy)
3. Based on findings, selectively run Phases 2-4 to test hypotheses
4. Use Phase 5 only if Phase 1 shows clear, publishable effect
5. Document findings with evidence from multi-dimensional testing

## Examples

See `examples/` directory:
- `phase1_authority_vs_sycophancy.rb` - Core mechanism testing
- `phase3_temperature_sensitivity.rb` - RLHF probability hypothesis
- `robustness_check_variator.rb` - Dialectical variation testing
